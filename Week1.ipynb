{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lORIgnt_yoZD"
   },
   "source": [
    "# **Week 1 : Linear Algebra and Calculus**\n",
    "\n",
    "In this assignment, we shall explore the concepts of analytic and numeric computation of gradients. Further, we will have a look at the computation graph which is a central concept to building a neural network. For learning how gradients are computed analytically, refer to the resources provided in this week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-z3YLRv1U_a"
   },
   "source": [
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQWsYeD8ZlYxFBB33qIn7bwQvP-KuvLZXJOoA&usqp=CAU\"\n",
    " style=\"float:center;width:50px;height:50px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nn_1mKR02Swb"
   },
   "source": [
    "# **Importing Libraries**\n",
    "Feel free to import any additional libraries required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "ugD-twoX2T4N"
   },
   "outputs": [],
   "source": [
    "# Import all libraries here\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Setting the seed for reproducible results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBmB8rFN2XT7"
   },
   "source": [
    "# *Problem 1(a)*\n",
    "\n",
    "In this problem, we shall be exploring the concepts of analytic and numeric computation of gradients for scalar valued functions. \n",
    "\n",
    "\\begin{equation}\n",
    "z = w^{T}x + b \\\\ \n",
    "\\hat{y} = \\sigma(z) = \\frac{1}{1+e^{-z}}\\\\ \n",
    "L(\\hat{y}, y) = y.log(\\hat{y}) + (1-y).log(1-\\hat{y})\n",
    "\\end{equation}\n",
    "\n",
    "For this set of equations, the vector w, and scalars b, y are to be treated as constants. \\\\\n",
    "\n",
    "Now, let us find the analytic gradient of the function L with respect to the function x. That is, compute $\\frac{\\delta L}{\\delta x}$. Write the answer obtained as code in the function provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qC-5BMVs-abp"
   },
   "source": [
    "Initialise $w$ to a $10 \\times 1$ vector sampled over a standard multivariate gaussian distribution, b to a uniform random variable over the interval $(0, 1)$, y to a bernoulli random variable over $\\{0, 1\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "id": "1z1FuiZn-mg1"
   },
   "outputs": [],
   "source": [
    "# Initialisation : \n",
    "\n",
    "w = np.random.multivariate_normal([0],[[1]],10)\n",
    "b = np.random.uniform(0, 1, 1)\n",
    "y = np.random.binomial(1, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "id": "rhgmU3v-6jhd"
   },
   "outputs": [],
   "source": [
    "def analytic_grad(x) : \n",
    "\n",
    "    ### WRITE CODE BELOW ###\n",
    "    z=np.dot(np.transpose(w), x)+b  #\n",
    "    y_cap=1/(1+np.exp(-z))\n",
    "    gradient=(y-y_cap)*np.transpose(w)\n",
    "    return gradient\n",
    "    ### WRITE CODE ABOVE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjAm_AKN7Qk-"
   },
   "source": [
    "# *Problem 1(b)*\n",
    "\n",
    "Now, we compute the numeric gradient for the function L. Refer to [this](https://stackoverflow.com/questions/38854363/is-there-any-standard-way-to-calculate-the-numerical-gradient) stack exchange post to see how numeric gradients are computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "id": "Ur4CfvOf7YeR"
   },
   "outputs": [],
   "source": [
    "def L(x):\n",
    "    return y*(np.log(1/(1+np.exp(-np.dot(np.transpose(w), x)+b))))+(1-y)*(np.log(1-(1/(1+np.exp(-np.dot(np.transpose(w), x)+b)))))\n",
    "\n",
    "def numeric_grad(x) : \n",
    "\n",
    "    ### WRITE CODE BELOW ###\n",
    "    eps=1e-8\n",
    "    h=(eps**0.5)*x\n",
    "    gradient=(L(x+h)-L(x-h))/2*h\n",
    "    \n",
    "    return gradient\n",
    "    ### WRITE CODE ABOVE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwJI9VYhLjCi"
   },
   "source": [
    "# *Problem 2*\n",
    "\n",
    "Here, we'll be looking at computational graphs, and their calculus, finding gradients of  variables with respect to other variables in the graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wwEXqQEREOH"
   },
   "source": [
    "We'll be looking at nodes of the graph that are operation based, i.e., each operation performed has a node associated with it.\n",
    "\n",
    "We'll provide you with example implementations for three of the nodes, the *add* node, the *nth power* node and the *sine* node, you'll have to write the classes for all the other nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIeEOTBytyGB"
   },
   "source": [
    "# **Multi-input nodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "JQuHj5IctK4N"
   },
   "outputs": [],
   "source": [
    "class Add: \n",
    "  \n",
    "  # A class to add multiple variables together\n",
    "  def __init__(self, lst_names, lst_values):\n",
    "    self.lst_names = lst_names # This numpy arr contains all the variable names that are to be added, with each variable name in datatype str.\n",
    "                               # Scalar addition is taken care of in a separate node, we could have fit into this node but thought it might be easier if it wasn't.\n",
    "                               # So cases like \"a + 1\" are to be done separately, and cases like \"b + c + d + 4\" can be done as \"(b + c + d) + 4\" or \"b + c + (d + 4)\", since our scalar addition supports only one variable and one scalar, we'll get to that later\n",
    "    self.lst_values = lst_values # This numpy arr contains all the variable values that are to be added, as scalars.\n",
    "\n",
    "  def compute_output(self):\n",
    "    return np.sum(self.lst_values) # This computes the sum of all the variables\n",
    "  \n",
    "  def compute_gradients(self):\n",
    "    return dict(zip(self.lst_names[0], np.ones(len(self.lst_names[0])))) # This gives the gradient of the sum wrt to all the input variables, as a dictionary, will be used later\n",
    "\n",
    "\n",
    "class Multiply: \n",
    "  \n",
    "  #Everything's almost the same as the add class, but this deals with multiplication of more than 1 variables\n",
    "  def __init__(self, lst_names, lst_values):\n",
    "    self.lst_names = lst_names # This numpy arr contains all the variable names that are to be multiplied, with each variable name in datatype str.\n",
    "                               # Scalar multiplication is taken care of in a separate node\n",
    "    self.lst_values = lst_values # This numpy arr contains all the variable values that are to be multiplied, as scalars.\n",
    "\n",
    "  def compute_output(self):\n",
    "    # Write your code to compute the output of this operation\n",
    "    return np.prod(self.lst_values[0])\n",
    "  \n",
    "  def compute_gradients(self):\n",
    "    # Write your code to create a dictionary, storing all the gradients of the output wrt to each input (In this case there is only a single input that matters)\n",
    "    gradients=[]\n",
    "    for i in range(len(self.lst_names[0])):\n",
    "        gradients.append(self.compute_output()/(self.lst_values[0][i]))\n",
    "    return dict(zip(self.lst_names[0], gradients))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QpfDyxht7Ob"
   },
   "source": [
    "# **Scalar multiplication/addition nodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "id": "-0RqnZrStbyI"
   },
   "outputs": [],
   "source": [
    "class Add_Scalar: \n",
    "  \n",
    "  # A class to add a variable with a scalar\n",
    "  def __init__(self, lst_names, lst_values):\n",
    "    self.lst_names = lst_names # This numpy arr contains all the variable names that are to be added, with each variable name in datatype str.\n",
    "                               # This list must only have 2 elements, the first should be the string corresponding to the name of the variable, and the second should be a string of the scalar value to be added.\n",
    "    self.lst_values = lst_values # This numpy arr contains all the variable values that are to be added, as scalars.\n",
    "\n",
    "  def compute_output(self):\n",
    "    # Write your code to compute the output of this operation\n",
    "    self.lst_names[1]=list(map(int, self.lst_names[1]))\n",
    "    arr=np.array([self.lst_values[0], self.lst_names[1]])\n",
    "    \n",
    "    return np.sum(arr, axis=0).tolist()\n",
    "\n",
    "  def compute_gradients(self):\n",
    "    # Write your code to create a dictionary, storing all the gradients of the output wrt to each input (In this case there is only a single input that matters)\n",
    "    return dict(zip(self.lst_names[0], np.ones(len(self.lst_values))))\n",
    "\n",
    "\n",
    "class Multiply_Scalar: \n",
    "  \n",
    "  # A class to multiply a variable with a scalar\n",
    "  def __init__(self, lst_names, lst_values):\n",
    "    self.lst_names = lst_names # This numpy arr contains all the variable names that are to be multiplied, with each variable name in datatype str.\n",
    "                               # This list must only have 2 elements, the first should be the string corresponding to the name of the variable, and the second should be a string of the scalar value to be multiplied.\n",
    "    self.lst_values = lst_values # This numpy arr contains all the variable values that are to be multiplied, as scalars.\n",
    "\n",
    "  def compute_output(self):\n",
    "    # Write your code to compute the output of this operation\n",
    "    out=self.lst_values[0][0]*float(self.lst_names[1][0])\n",
    "    return out\n",
    "\n",
    "  def compute_gradients(self):\n",
    "    # Write your code to create a dictionary, storing all the gradients of the output wrt to each input (In this case there is only a single input that matters)\n",
    "    return {self.lst_names[0][0]: float(self.lst_names[1][0])}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iF4EA5CLuKXR"
   },
   "source": [
    "# **Nodes for special functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "RzhhbukvRBD-"
   },
   "outputs": [],
   "source": [
    "class Power:\n",
    "\n",
    "  def __init__(self, name, value, exponent):\n",
    "    self.name = name # Name of the variable to be exponentiated\n",
    "    self.value = value # Value of the variable\n",
    "    self.exponent = exponent # Value of the exponent\n",
    "  \n",
    "  def compute_output(self):\n",
    "    return np.power(self.value, self.exponent)\n",
    "  \n",
    "  def compute_gradients(self):\n",
    "    return {self.name : self.exponent*(np.power(self.value, (self.exponent - 1)))}\n",
    "\n",
    "class Sine: \n",
    "\n",
    "  # A class to apply the sine function on a variable\n",
    "  def __init__(self, name, value):\n",
    "    self.name = name\n",
    "    self.value = value\n",
    "  \n",
    "  def compute_output(self):\n",
    "    return np.sin(self.value)\n",
    "  \n",
    "  def compute_gradients(self):\n",
    "    return {self.name : np.cos(self.value)}\n",
    "\n",
    "\n",
    "class Logarithm:\n",
    "\n",
    "  # A class to compute the logarithm of a value\n",
    "  def __init__(self, name, value):\n",
    "    self.name = name\n",
    "    self.value = value\n",
    "  \n",
    "  def compute_output(self):\n",
    "    # Write your code here\n",
    "    return np.log(self.value)\n",
    "\n",
    "  def compute_gradients(self):\n",
    "    # Write your code here\n",
    "    return {self.name: 1/(self.value)}\n",
    "\n",
    "class Exponential: \n",
    "\n",
    "  # A class to compute the exponential of a value\n",
    "  def __init__(self, name, value):\n",
    "    self.name = name\n",
    "    self.value = value\n",
    "  \n",
    "  def compute_output(self):\n",
    "    # Write your code here\n",
    "    return np.exp(self.value)\n",
    "\n",
    "  def compute_gradients(self):\n",
    "    # Write your code here\n",
    "    return {self.name: np.exp(self.value)} \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsUmSRPKXau8"
   },
   "source": [
    "Now that we have these classes, let's use them to actually find gradients of complex networks. We finally bring in the idea of a computational graph, which makes it much easier for the gradients to be computed.\n",
    "\n",
    "This is the image of the example graph that we want you to work with. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2hDABSUr8Px"
   },
   "source": [
    "<div>\n",
    "<img src=\"https://drive.google.com/uc?id=1VtI1lf85bG8cO1u_8l0D0rqVGhwHQtPK\"\n",
    " width=\"500\" height=\"500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPKf5T4lrjxY"
   },
   "source": [
    "\\begin{equation}\n",
    "d = 3a \\\\ \n",
    "e = abc \\\\ \n",
    "f = sin(c) \\\\ \n",
    "g = exp(e) \\\\ \n",
    "h = a + d + g + f\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnOQqmnlw7eN"
   },
   "source": [
    "# **Forward Propogation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "id": "GM2n6S4sw-r_"
   },
   "outputs": [],
   "source": [
    "def forward_prop(a, b, c) : \n",
    "    '''\n",
    "    Input : real numbers a, b, c.\n",
    "\n",
    "    Outputs : A dictionary of the values at each node with keys as the names of nodes\n",
    "    Grads : A dictionary of the gradients at each edge with keys as a pair of nodes \n",
    "    \n",
    "    e.g. Grads[\"ce\"] = ...\n",
    "    '''\n",
    "    Outputs={'a': a, 'b': b, 'c': c}\n",
    "    Grads={'aa': 1, 'bb': 1, 'cc': 1}\n",
    "    multiply_scalar=Multiply_Scalar([[\"d\"],[\"3\"]], [[a]])\n",
    "    Outputs['d']=multiply_scalar.compute_output()\n",
    "    Grads={**Grads, **{'da': multiply_scalar.compute_gradients()['d']}}\n",
    "    multiply=Multiply([['a', 'b', 'c']], [[a, b, c]])\n",
    "    Outputs['e']=multiply.compute_output()\n",
    "    x=multiply.compute_gradients()\n",
    "    x['ea']=x.pop('a')\n",
    "    x['eb']=x.pop('b')\n",
    "    x['ec']=x.pop('c')\n",
    "    Grads={**Grads, **x}\n",
    "    del x\n",
    "    sine=Sine('f', c)\n",
    "    Outputs['f']=sine.compute_output()\n",
    "    Grads={**Grads, **sine.compute_gradients()}\n",
    "    Grads['fc']=Grads.pop('f')\n",
    "    exp=Exponential('g', Outputs['e'])\n",
    "    Outputs['g']=exp.compute_output()\n",
    "    Grads={**Grads, **exp.compute_gradients()}\n",
    "    Grads['ge']=Grads.pop('g')\n",
    "    add=Add([[\"a\", \"d\", \"g\", \"f\"]], [[a, Outputs['d'], Outputs['g'], Outputs['f']]])\n",
    "    Outputs[\"h\"]=add.compute_output()\n",
    "    d=add.compute_gradients()\n",
    "    d['ha']=d.pop('a')\n",
    "    d['hd']=d.pop('d')\n",
    "    d['hg']=d.pop('g')\n",
    "    d['hf']=d.pop('f')\n",
    "    Grads={**Grads, **d}\n",
    "    return (Outputs, Grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApNrGkHJxjMw"
   },
   "source": [
    "# **Backward Propogation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "450-YOv8fqmR"
   },
   "source": [
    "Most of the usage of computational graphs in Machine Learning centers around finding the gradients of a particular loss, wrt to all the parameters. Here, your task is to do a simpler version of that.\n",
    "\n",
    "Use the classes you wrote to calculate the following gradients : \n",
    "\n",
    "\n",
    "*   $ \\frac{\\partial h}{\\partial a}$\n",
    "*   $ \\frac{\\partial h}{\\partial b}$\n",
    "*   $ \\frac{\\partial h}{\\partial c}$\n",
    "\n",
    "Use the technique of *backpropogation* to code out the gradients step-wise, along all possible chains of the graph starting from $h$ and ending at $a,b,c$ respectively. \n",
    "\n",
    "Don't try to directly get these values without backpropogation, it might be easier here, but with complicated neural networks it wouldn't be :) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "id": "YwL98tWexpGy"
   },
   "outputs": [],
   "source": [
    "def all_paths(adj_list, start, end, index):\n",
    "    def dfs(start, index, path, output):\n",
    "        if start==end:\n",
    "            output.append(path)\n",
    "        for i in adj_list[start]:\n",
    "            dfs(i, index, path+[i], output)\n",
    "    output=[]\n",
    "    dfs('h', index, ['h'], output)\n",
    "    return output\n",
    "    \n",
    "\n",
    "def backward_prop(a, b, c, Outputs, Grads) : \n",
    "    '''\n",
    "    Input : a, b, c (input layer); Outputs (values at each node); Grads (gradients stored at each edge)\n",
    "    Retuns : result (gradients w.r.t a, b, c)\n",
    "    '''\n",
    "    h_a=[]\n",
    "    h_b=[]\n",
    "    h_c=[]\n",
    "    result=[]\n",
    "    grad_keys=list(Grads.keys())\n",
    "    \n",
    "#Creating Adjacency List\n",
    "\n",
    "    adj_list={'a': [], 'b': [], 'c': []}\n",
    "    v=['d', 'e', 'f', 'g', 'h']\n",
    "    for i in v:\n",
    "        l=[]\n",
    "        for j in grad_keys:\n",
    "            if j[0]==i:\n",
    "                l.append(j[1])\n",
    "        adj_list={**adj_list, **{i: l}}\n",
    "\n",
    "    index={'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7}\n",
    "    h_a=all_paths(adj_list, 'h', 'a', index)\n",
    "    h_b=all_paths(adj_list, 'h', 'b', index)\n",
    "    h_c=all_paths(adj_list, 'h', 'c', index)\n",
    "    \n",
    "#Restructuring h_a, h_b and h_c and appending gradients to result\n",
    "\n",
    "    for i in h_a:\n",
    "        for j in range(len(i)-1):\n",
    "            i[j]=i[j]+i[j+1]\n",
    "        del i[j+1]\n",
    "    s=0\n",
    "    for i in h_a:\n",
    "        if(len(i)!=1):\n",
    "            t=1\n",
    "            for j in i:\n",
    "                t=t*Grads[j]\n",
    "        else:\n",
    "            t=Grads[i[0]]\n",
    "        s=s+t\n",
    "    result.append(s)\n",
    "    \n",
    "    for i in h_b:\n",
    "        for j in range(len(i)-1):\n",
    "            i[j]=i[j]+i[j+1]\n",
    "        del i[j+1]\n",
    "    s=0\n",
    "    for i in h_b:\n",
    "        if(len(i)!=1):\n",
    "            t=1\n",
    "            for j in i:\n",
    "                t=t*Grads[j]\n",
    "        else:\n",
    "            t=Grads[i[0]]\n",
    "        s=s+t\n",
    "    result.append(s)\n",
    "    \n",
    "    for i in h_c:\n",
    "        for j in range(len(i)-1):\n",
    "            i[j]=i[j]+i[j+1]\n",
    "        del i[j+1]\n",
    "    s=0\n",
    "    for i in h_c:\n",
    "        if(len(i)!=1):\n",
    "            t=1\n",
    "            for j in i:\n",
    "                t=t*Grads[j]\n",
    "        else:\n",
    "            t=Grads[i[0]]\n",
    "        s=s+t\n",
    "    result.append(s)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOsnYr0lx_KT"
   },
   "source": [
    "# **Combining both processes**\n",
    "\n",
    "For the purpose of values, assume that $a = 3, b = 1, c = 2$. Here, we call both forward and backward propogation functions to demonstrate functionality of the functions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "id": "cn-KFyH4yWAJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value obtained upon forward propogation : 416.3380909195608\n",
      "Values of gradients are : 810.8575869854702, 2420.5727609564105, 1209.8702336416582\n"
     ]
    }
   ],
   "source": [
    "# Initialisation\n",
    "a = 3\n",
    "b = 1\n",
    "c = 2\n",
    "\n",
    "# Forward propogation\n",
    "(Outputs, Grads) = forward_prop(a, b, c)\n",
    "print(f\"Value obtained upon forward propogation : {Outputs['h']}\")\n",
    "\n",
    "# Backward propogation\n",
    "result = backward_prop(a, b, c, Outputs, Grads)\n",
    "print(f'Values of gradients are : {result[0]}, {result[1]}, {result[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMTRGZQoy-VU"
   },
   "source": [
    "# **Submission Instructions**\n",
    "\n",
    "Upload this notebook on your github classroom repository by the name Week1.ipynb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
